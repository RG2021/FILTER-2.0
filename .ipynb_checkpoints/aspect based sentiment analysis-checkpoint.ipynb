{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import stanfordnlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device: cpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': 'C:\\\\Users\\\\HP\\\\stanfordnlp_resources\\\\en_ewt_models\\\\en_ewt_tokenizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: pos\n",
      "With settings: \n",
      "{'model_path': 'C:\\\\Users\\\\HP\\\\stanfordnlp_resources\\\\en_ewt_models\\\\en_ewt_tagger.pt', 'pretrain_path': 'C:\\\\Users\\\\HP\\\\stanfordnlp_resources\\\\en_ewt_models\\\\en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: lemma\n",
      "With settings: \n",
      "{'model_path': 'C:\\\\Users\\\\HP\\\\stanfordnlp_resources\\\\en_ewt_models\\\\en_ewt_lemmatizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Building an attentional Seq2Seq model...\n",
      "Using a Bi-LSTM encoder\n",
      "Using soft attention for LSTM.\n",
      "Finetune all embeddings.\n",
      "[Running seq2seq lemmatizer with edit classifier]\n",
      "---\n",
      "Loading: depparse\n",
      "With settings: \n",
      "{'model_path': 'C:\\\\Users\\\\HP\\\\stanfordnlp_resources\\\\en_ewt_models\\\\en_ewt_parser.pt', 'pretrain_path': 'C:\\\\Users\\\\HP\\\\stanfordnlp_resources\\\\en_ewt_models\\\\en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Done loading processors!\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "nlp = stanfordnlp.Pipeline(use_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use device: cpu\n",
      "---\n",
      "Loading: tokenize\n",
      "With settings: \n",
      "{'model_path': 'C:\\\\Users\\\\HP\\\\stanfordnlp_resources\\\\en_ewt_models\\\\en_ewt_tokenizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: pos\n",
      "With settings: \n",
      "{'model_path': 'C:\\\\Users\\\\HP\\\\stanfordnlp_resources\\\\en_ewt_models\\\\en_ewt_tagger.pt', 'pretrain_path': 'C:\\\\Users\\\\HP\\\\stanfordnlp_resources\\\\en_ewt_models\\\\en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "---\n",
      "Loading: lemma\n",
      "With settings: \n",
      "{'model_path': 'C:\\\\Users\\\\HP\\\\stanfordnlp_resources\\\\en_ewt_models\\\\en_ewt_lemmatizer.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Building an attentional Seq2Seq model...\n",
      "Using a Bi-LSTM encoder\n",
      "Using soft attention for LSTM.\n",
      "Finetune all embeddings.\n",
      "[Running seq2seq lemmatizer with edit classifier]\n",
      "---\n",
      "Loading: depparse\n",
      "With settings: \n",
      "{'model_path': 'C:\\\\Users\\\\HP\\\\stanfordnlp_resources\\\\en_ewt_models\\\\en_ewt_parser.pt', 'pretrain_path': 'C:\\\\Users\\\\HP\\\\stanfordnlp_resources\\\\en_ewt_models\\\\en_ewt.pretrain.pt', 'lang': 'en', 'shorthand': 'en_ewt', 'mode': 'predict'}\n",
      "Done loading processors!\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import stanfordnlp\n",
    "nlp = stanfordnlp.Pipeline(use_gpu=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem.wordnet import WordNetLemmatizer "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import stanfordnlp\n",
    "\n",
    "def aspect_sentiment_analysis(txt):\n",
    "    txt = txt.lower()\n",
    "    sentList = nltk.sent_tokenize(txt)\n",
    "\n",
    "    fcluster = []\n",
    "    totalfeatureList = []\n",
    "    finalcluster = []\n",
    "    dic = {}\n",
    "\n",
    "    for line in sentList:\n",
    "        newtaggedList = []\n",
    "        txt_list = nltk.word_tokenize(line)\n",
    "        taggedList = nltk.pos_tag(txt_list)\n",
    "\n",
    "        newwordList = []\n",
    "        flag = 0\n",
    "        for i in range(0,len(taggedList)-1):\n",
    "            if(taggedList[i][1]==\"NN\" and taggedList[i+1][1]==\"NN\"):\n",
    "                newwordList.append(taggedList[i][0]+taggedList[i+1][0])\n",
    "                flag=1\n",
    "            else:\n",
    "                if(flag==1):\n",
    "                    flag=0\n",
    "                    continue\n",
    "                newwordList.append(taggedList[i][0])\n",
    "                if(i==len(taggedList)-2):\n",
    "                    newwordList.append(taggedList[i+1][0])\n",
    "\n",
    "        finaltxt = ' '.join(word for word in newwordList)\n",
    "        new_txt_list = nltk.word_tokenize(finaltxt)\n",
    "        wordsList = [w for w in new_txt_list if not w in stop_words]\n",
    "        taggedList = nltk.pos_tag(wordsList)\n",
    "\n",
    "        doc = nlp(finaltxt)\n",
    "\n",
    "        dep_node = []\n",
    "        for dep_edge in doc.sentences[0].dependencies:\n",
    "            dep_node.append([dep_edge[2].text, dep_edge[0].index, dep_edge[1]])\n",
    "\n",
    "        for i in range(0, len(dep_node)):\n",
    "            if (int(dep_node[i][1]) != 0):\n",
    "                dep_node[i][1] = newwordList[(int(dep_node[i][1]) - 1)]\n",
    "\n",
    "        featureList = []\n",
    "        for i in taggedList:\n",
    "            if(i[1]=='JJ' or i[1]=='NN' or i[1]=='JJR' or i[1]=='NNS' or i[1]=='RB'):\n",
    "                featureList.append(list(i))\n",
    "                totalfeatureList.append(list(i))\n",
    "\n",
    "        categories = []\n",
    "        for i in featureList:\n",
    "            categories.append(i[0])\n",
    "\n",
    "        for i in featureList:\n",
    "            filist = []\n",
    "            for j in dep_node:\n",
    "                if((j[0]==i[0] or j[1]==i[0]) and (j[2] in [\"nsubj\", \"acl:relcl\", \"obj\", \"dobj\", \"agent\", \"advmod\", \"amod\", \"neg\", \"prep_of\", \"acomp\", \"xcomp\", \"compound\"])):\n",
    "                    if(j[0]==i[0]):\n",
    "                        filist.append(j[1])\n",
    "                    else:\n",
    "                        filist.append(j[0])\n",
    "            fcluster.append([i[0], filist])\n",
    "            \n",
    "    for i in totalfeatureList:\n",
    "        dic[i[0]] = i[1]\n",
    "    \n",
    "    for i in fcluster:\n",
    "        if(dic[i[0]]==\"NN\"):\n",
    "            finalcluster.append(i)\n",
    "        \n",
    "    return(finalcluster)\n",
    "\n",
    "\n",
    "\n",
    "def tagging(arr):\n",
    "    \n",
    "    nlp = stanfordnlp.Pipeline()\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    countlist = {}\n",
    "    finallist = {}\n",
    "    for i in range(0, len(arr)):\n",
    "        txt = arr[i]\n",
    "        l = aspect_sentiment_analysis(txt)\n",
    "        for j in l:\n",
    "            if(j[0] not in finallist):\n",
    "                countlist[j[0]] = 1\n",
    "                for k in j[1]:\n",
    "                    if(k in pos_words):\n",
    "                        finallist[j[0]] = 1\n",
    "                    elif(k in neg_words):\n",
    "                        finallist[j[0]] = 0\n",
    "                        \n",
    "            else:\n",
    "                countlist[j[0]] = countlist[j[0]] + 1\n",
    "                for k in j[1]:\n",
    "                    if(k in pos_words):\n",
    "                        finallist[j[0]] = finallist[j[0]]+1\n",
    "                    elif(k in neg_words):\n",
    "                        finallist[j[0]] = finallist[j[0]]+0\n",
    "                        \n",
    "    tag_rating_list = []\n",
    "    for i in finallist:\n",
    "        tag_rating_list.append([i, (finallist[i]/countlist[i])])\n",
    "\n",
    "    return (tag_rating_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\\aten\\src\\ATen\\native\\LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "..\\aten\\src\\ATen\\native\\LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
     ]
    }
   ],
   "source": [
    "text = \"The battery was awesome and amazing but the display is totally bad. The processor is good.\"\n",
    "\n",
    "finalcluster= aspect_sentiment_analysis(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['battery', ['awesome']],\n",
       " ['display', ['bad']],\n",
       " ['processor', ['good']],\n",
       " ['good', ['processor']]]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "finalcluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['battery', 'NN'],\n",
       " ['awesome', 'JJ'],\n",
       " ['amazing', 'JJ'],\n",
       " ['display', 'NN'],\n",
       " ['totally', 'RB'],\n",
       " ['bad', 'JJ'],\n",
       " ['processor', 'NN'],\n",
       " ['also', 'RB'],\n",
       " ['good', 'JJ']]"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "totalfeatureList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pos_path = \"C:/Users/HP/Desktop/FILTER/amazon_reviews_scraping/positive-words.txt\"\n",
    "neg_path = \"C:/Users/HP/Desktop/FILTER/amazon_reviews_scraping/negative-words.txt\"\n",
    "\n",
    "pos = pd.read_csv(pos_path, sep=\"\\n\", header=None, encoding='latin-1')\n",
    "neg = pd.read_csv(neg_path, sep=\"\\n\", header=None, encoding='latin-1')\n",
    "\n",
    "pos_words = np.array(pos[0]).tolist()\n",
    "neg_words = np.array(neg[0]).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tagging(arr):\n",
    "    countlist = {}\n",
    "    finallist = {}\n",
    "    for i in range(0, len(arr)):\n",
    "        txt = arr[i]\n",
    "        l = aspect_sentiment_analysis(txt)\n",
    "        for j in l:\n",
    "            if(j[0] not in finallist):\n",
    "                countlist[j[0]] = 1\n",
    "                for k in j[1]:\n",
    "                    if(k in pos_words):\n",
    "                        finallist[j[0]] = 1\n",
    "                    elif(k in neg_words):\n",
    "                        finallist[j[0]] = 0\n",
    "                        \n",
    "            else:\n",
    "                countlist[j[0]] = countlist[j[0]] + 1\n",
    "                for k in j[1]:\n",
    "                    if(k in pos_words):\n",
    "                        finallist[j[0]] = finallist[j[0]]+1\n",
    "                    elif(k in neg_words):\n",
    "                        finallist[j[0]] = finallist[j[0]]+0\n",
    "\n",
    "    return (finallist, countlist)\n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = [\"The battery was awesome and amazing but the display is totally bad. The processor is good.\", \"It was good\", \"I like the battery. Display is worst\", \"The battery was awesome and amazing and the display is also best.\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\\aten\\src\\ATen\\native\\LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "..\\aten\\src\\ATen\\native\\LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "..\\aten\\src\\ATen\\native\\LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "..\\aten\\src\\ATen\\native\\LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "..\\aten\\src\\ATen\\native\\LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n",
      "..\\aten\\src\\ATen\\native\\LegacyDefinitions.cpp:19: UserWarning: masked_fill_ received a mask with dtype torch.uint8, this behavior is now deprecated,please use a mask with dtype torch.bool instead.\n"
     ]
    }
   ],
   "source": [
    "tagratinglist, tagcountlist = tagging(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'battery': 3, 'display': 1, 'processor': 1}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagratinglist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'battery': 3, 'display': 3, 'processor': 1, 'good': 1}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagcountlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the', 'battery', 'det'],\n",
       " ['battery', 'awesome', 'nsubj'],\n",
       " ['was', 'awesome', 'cop'],\n",
       " ['awesome', '0', 'root'],\n",
       " ['and', 'amazing', 'cc'],\n",
       " ['amazing', 'awesome', 'conj'],\n",
       " ['but', 'bad', 'cc'],\n",
       " ['the', 'display', 'det'],\n",
       " ['display', 'bad', 'nsubj'],\n",
       " ['is', 'bad', 'cop'],\n",
       " ['totally', 'bad', 'advmod'],\n",
       " ['bad', 'awesome', 'conj'],\n",
       " ['.', 'awesome', 'punct']]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('battery', 'NN'),\n",
       " ('awesome', 'JJ'),\n",
       " ('display', 'NN'),\n",
       " ('bad', 'JJ'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the', '2', 'det'],\n",
       " ['batterylife', '5', 'nsubj'],\n",
       " ['is', '5', 'cop'],\n",
       " ['just', '5', 'advmod'],\n",
       " ['great', '0', 'root']]"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dep_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(0, len(dep_node)):\n",
    "    if (int(dep_node[i][1]) != 0):\n",
    "        dep_node[i][1] = newwordList[(int(dep_node[i][1]) - 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['the', 'batterylife', 'det'],\n",
       " ['batterylife', 'great', 'nsubj'],\n",
       " ['is', 'great', 'cop'],\n",
       " ['just', 'great', 'advmod'],\n",
       " ['great', '0', 'root']]"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dep_node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('batterylife', 'NN'), ('great', 'JJ')]"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taggedList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "featureList = []\n",
    "for i in taggedList:\n",
    "    if(i[1]=='JJ' or i[1]=='NN' or i[1]=='JJR' or i[1]=='NNS'):\n",
    "        featureList.append(list(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['batterylife', 'NN'], ['great', 'JJ']]"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featureList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = []\n",
    "for i in featureList:\n",
    "    categories.append(i[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['batterylife', 'great']"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "fcluster = []\n",
    "for i in featureList:\n",
    "    filist = []\n",
    "    for j in dep_node:\n",
    "        if((j[0]==i[0] or j[1]==i[0]) and (j[2] in [\"nsubj\", \"dobj\", \"agent\", \"advmod\", \"amod\", \"neg\", \"prep_of\", \"acomp\", \"xcomp\", \"compound\"])):\n",
    "            if(j[0]==i[0]):\n",
    "                filist.append(j[1])\n",
    "            else:\n",
    "                filist.append(j[0])\n",
    "    fcluster.append([i[0], filist])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['batterylife', ['great']], ['great', ['batterylife', 'just']]]"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fcluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = {}\n",
    "for i in fcluster:\n",
    "    dic[i[0]] = i[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batterylife': ['great'], 'great': ['batterylife', 'just']}"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
